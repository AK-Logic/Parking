{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f19d3a33",
   "metadata": {},
   "source": [
    "# Parking Sign Recognition\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a8608e",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. pictures of every possible sign [doesn't need words]\n",
    "2. need to find python library to do the following a) Recognize the sign object b) match the sign with our database of base signs c) read the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96ea567",
   "metadata": {},
   "source": [
    "2a & b) opencv python (cv2)\n",
    "2c) Pytesseract-ocr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd94ed6",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7f88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image detection\n",
    "import cv2 #opencv is cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "\n",
    "# reading text\n",
    "import pytesseract\n",
    "import PIL.Image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a901470",
   "metadata": {},
   "source": [
    "# Reading and Displaying Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb4067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube tut: https://www.youtube.com/watch?v=T-0lZWYWE9Y&list=PLzMcBGfZo4-lUA8uGjeXhBUUzPYc6vZRn&index=7\n",
    "\n",
    "# Loading Images\n",
    "# imread has a second paramter of -1, 0 or 1\n",
    "# -1: loads a colored image, neglecting transparency (default)\n",
    "# 0: loads image in grayscale\n",
    "# 1: includes image as it is, including transparency\n",
    "\n",
    "imgt1 = cv2.imread('Parking Signs/Base Signs/no standing.jpg', 0) #imgt1 for image template1\n",
    "img1 = cv2.imread('Parking Signs/Signs/cams house.jpg', 0) #img1, this is the image we will run template again.\n",
    "img1 = cv2.resize(img1, (800,800)) # This image was way too big initially\n",
    "\n",
    "# These next 3 lines displays the image\n",
    "cv2.imshow('template1',imgt1) # you need the next two lines to run this\n",
    "cv2.waitKey(0) # Waits a number of seconds until we press a key to move onto the next line, 0 indicates to wait infinitely\n",
    "cv2.destroyAllWindows() # closes the image\n",
    "cv2.imshow('image1',img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n",
    "\n",
    "\n",
    "# images are understood by python as numpy arrays representing 3 values per pixel [blue, green, red] where blue green red represent a number from 0-255\n",
    "# the columns represent the width while the rows represent the height"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5c280dd",
   "metadata": {},
   "source": [
    "# Template Matching Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aac2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To recognize a sign, we can use corner detection (shi-thomasi Detector & goodFeaturesToTrack()). This may be problematic by picking up other corners in an image\n",
    "# however this does allow you to specify a few paramaters to help improve accuracy. \n",
    "# Paramaters are: (input image, # of best corners to choose, confidence level, min euclidean distance between corners)\n",
    "\n",
    "# Template matching is likely the algorithm to use. A potential issue is that the size of the template image needs to be close enough to the size in the actual image.\n",
    "# In our case we are going to try and match pictures in Base Signs to picutres in Signs folder.\n",
    "# This algorithm requires grayscale. Note that since we are using grayscale now instead of a 3 dimensional array [height, width, channel], its just [height, width]\n",
    "# 6 different methods to template match. It is recommended to try each method and figure out which gets the best results.\n",
    "# The methods are TM_CCOEFF, TM_CCOEFF_NORMED, TM_CCORR, TM_CCORR_NORMED, TM_SQDIFF, TM_SQDIFF_NORMED\n",
    "\n",
    "methods = [cv2.TM_CCOEFF, cv2.TM_CCOEFF_NORMED, cv2.TM_CCORR, cv2.TM_CCORR_NORMED, cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]\n",
    "\n",
    "h, w = imgt1.shape\n",
    "\n",
    "# loop through all methods and find the best one\n",
    "for method in methods:\n",
    "    img2 = img1.copy()\n",
    "    \n",
    "    result = cv2.matchTemplate(img2, imgt1, method) # this performs a convolution. In other words, it slides img2 all around imgt1 until it finds a reasonable match.\n",
    "                                            # this returns a 2 dimension array telling us how confident of a match there is in each region of the image\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result) # we case about min_loc, max_loc. These are the locations with best match. Max or min is best, depending on the method.\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        location = min_loc\n",
    "    else:\n",
    "        location = max_loc\n",
    "\n",
    "    bottom_right = (location[0] + w, location[1] + h)\n",
    "    cv2.rectangle(img2, location, bottom_right, 255, 5) # We draw a rectangle at the location, 255 for black, line thickness 5\n",
    "    cv2.imshow('Match', img2)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows() # Seems like method 1 is the only one that works for this example. Size of both images is very important. Need to figure out best sizing.\n",
    "\n",
    "\n",
    "    # Ideally, once we draw a rectangle around the image, we crop it and feed it into pytesseract to read the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6f043",
   "metadata": {},
   "source": [
    "# Canny Method of Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44551fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import numpy as np\n",
    "#import cv2\n",
    "#import imutils\n",
    "\n",
    "image = cv2.imread('Parking Signs/Signs/contradictory_parkingsigns.jpeg')\n",
    "\n",
    "def find_edges(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    edged = cv2.Canny(image=gray, threshold1=150, threshold2=175)\n",
    "    cnts = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "\n",
    "    cv2.imshow('edged',edged) # you need the next two lines to run this\n",
    "    cv2.waitKey(0) # Waits a number of seconds until we press a key to move onto the next line, 0 indicates to wait infinitely\n",
    "    cv2.destroyAllWindows() # closes the image\n",
    "\n",
    "\n",
    "find_edges(image)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7d457cb",
   "metadata": {},
   "source": [
    "# Attempting to Read signs using Pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6655d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1\n",
      "S\n",
      "\n",
      "M- PM\n",
      "\n",
      "MON-FRI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in some base signs\n",
    "imgt1 = cv2.imread('Parking Signs/Base Signs/no standing.jpg', 0)\n",
    "imgt2 = cv2.imread('Parking Signs/Base Signs/can park here.jpg', 0)\n",
    "imgt3 = cv2.imread('Parking Signs/Base Signs/no parking anytime.png', 0)\n",
    "imgt4 = cv2.imread('Parking Signs/Base Signs/no parking by time2.png', 0)\n",
    "\n",
    "# pytesseract has a engine mode and page segmentation mode to choose for specific situations for more accuracy\n",
    "\n",
    "# Page segmentation modes:\n",
    "# 0. Orientation and script detection (OSD) only.\n",
    "# 1. Automatic page segmentation with OSD.\n",
    "# 2. Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
    "# 3. Fully automatic page segmentation, but no OSD. (Default)\n",
    "# 4. Assume a single column of text of variable sizes.\n",
    "# 5. Assume a single uniform block of vertically aligned text.\n",
    "# 6. Assume a single uniform block of text.\n",
    "# 7. Treat the image as a single text line.\n",
    "# 8. Treat the image as a single word.\n",
    "# 9. Treat the image as a single word in a circle.\n",
    "# 10. Treat the image as a single character.\n",
    "# 11. Sparse text. Find as much text as possible in no particular order.\n",
    "# 12. Sparse text with OSD.\n",
    "# 13. Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "\n",
    "# OCR Engine modes:\n",
    "# 0. Legacy engine only.\n",
    "# 1. Neural nets LSTM engine only.\n",
    "# 2. Legacy + LSTM engines.\n",
    "# 3. Default, based on what is available.\n",
    "\n",
    "# Lets try this configuration\n",
    "myconfig = r\"--psm 11 --oem 3\"\n",
    "\n",
    "# Here is a function where we can draw boxes around the characters recognized by tesseract\n",
    "def box_char(img):\n",
    "    boxes = pytesseract.image_to_boxes(img)\n",
    "    h, w, z = img.shape\n",
    "    for box in boxes.splitlines():\n",
    "        box = box.split(\" \")\n",
    "        img = cv2.rectangle(img, int(box[1]), h - int(box[2]), int(box[3]), h - int(box[4]), (0,255,0), 2)\n",
    "    cv2.WaitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "print(\"image1\")\n",
    "text = pytesseract.image_to_string(imgt1, config = myconfig)\n",
    "print(text)\n",
    "cv2.imshow('template1',imgt1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "box_char(imgt1)\n",
    "\n",
    "\n",
    "print(\"image2\")\n",
    "text2 = pytesseract.image_to_string(imgt2, config = myconfig)\n",
    "print(text2)\n",
    "cv2.imshow('template1',imgt2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"image3\")\n",
    "text3 = pytesseract.image_to_string(imgt3, config = myconfig)\n",
    "print(text3)\n",
    "cv2.imshow('template1',imgt3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"image4\")\n",
    "text4 = pytesseract.image_to_string(imgt4, config = myconfig)\n",
    "print(text4)\n",
    "cv2.imshow('template1',imgt4)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f91d42f53e80f5ea6d888450c870edf2c47869be918694b1642fc472cb88b6bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
